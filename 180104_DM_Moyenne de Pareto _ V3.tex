\documentclass[a4paper,11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{amsmath} 
\usepackage{dsfont}
\usepackage{amsthm,amsfonts,amssymb,epsfig,titling,caption,graphicx,url,array}
\usepackage[left=3cm,right=2cm,top=2cm,bottom=2cm]{geometry}

%epstopdf
%\begin{document}
%\theoremstyle{defintion}
\theoremstyle{remark}  
%\newtheorem{Rem}{Remarque}[section]
%\newtheorem{avert}{Avertissement} 


\begin{document}
\noindent\Large{Mai 2016 - Master 1 MAEF}
\hspace{27mm}
\Large{Universit\'e Paris 1}

\vspace{70mm}
\begin{center} \textsc{\Huge{T.E.R. \\ S\'eries de Fourier, Exemples et Applications}}\\
\vspace{5mm}
\Large{Sous la direction de M.Pennequin}
\end{center}
\vspace{80mm}
\begin{flushright}\Large{Cl\'ementine Rosier}
\end{flushright}
\newpage

\subsubsection*{Question 2}
Cherchons tout d'abord si la loi de pareto admet une esp\'erance et si oui, sous quelle conditions. \\

Soit $c\in\mathbb{R}$, on a: \\

\begin{align*}
\mathbb{E}(x)&= \int_{-\infty}^{+\infty}|x|\theta\frac{c^{\theta}}{x^{\theta-1}} \mathds{1}_{x>c} dx \\
			 &= \theta c^{\theta}  \int_{c}^{+\infty} \frac{|x|}{x^{\theta + 1}} dx
\end{align*}

 Or on sait que pour tout $t\leq0$:\\
 \begin{center}
 	$\int_{t}^{+\infty}\frac{1}{x^{\theta}}dx$ converge si et seulement si $ \theta>1$.
 \end{center}
Ainsi la loi de Pareto admet une esp\'erance si et seulement $\theta>1$.\\
\\
Alors pour tout $c$ r\'eel et pour tout $\theta$ strictement sup\'erieur \`a 1, on peut calculer:\\

\begin{align*}
\mathbb{E}(x)&= \int_{c}^{+\infty} \theta c^{\theta} \frac{1}{x^{\theta}}dx\\
			&= \theta c^{\theta}\times [-\frac{1}{1+\theta}x^{1+\theta}]^{+\infty}_{c}\\
			&= \theta c^{\theta} \frac{-1}{1 - \theta}\times c^{1-\theta} +0\\
\textnormal{Ainsi, } \mathbb{E}(x)&= \frac{\theta}{\theta -1}c
\end{align*}

\section*{PARTIE I}
\subsection*{A. Maximum de vraisemblance}
\subsubsection*{Question 1}
Calculons tout d'abord la vraisemblance. On a, comme les n observations sont ind\'ependantes et identiquement distrbu\'ees:\\
$$L_{n}(x_{1};...;x_{n};\theta)= \prod_{i=1}^{n} \frac{\theta c^{\theta}}{x_{i}^{\theta +1}} = \theta^{n}c^{\theta n}\times ( \prod_{i=1}^{n} x_{i})^{-(\theta +1)}$$
On passe \`a la log-vraisemblance pour faciliter ensuite la maximisation. En effet, la fonction logarithme \'etant croissante, les deux maximisation sont \'equivalentes. On obtient alors:
$$l_{n}(x_{1};...;x_{n};\theta)= n\ln\theta + n\theta\ln(c)-(\theta+1)\sum_{i=1}^{n}\ln(x_{i})$$

La condition de premier ordre, nous donne alors:
$$\frac{\partial l_{n}}{\partial \theta}(x_{1};...;x_{n};\theta)=0 $$
i.e. $$\frac{n}{\theta} + n\ln(c) - \sum_{i=1}^{n}\ln(x_{i})=0$$
i.e.$$ \frac{n}{\theta}= \sum{i=1}^{n}\ln(x_{i})-n\ln(c)$$
i.e.$$\theta = \frac{n}{\sum_{i=1}^{n}\ln(\frac{x_{i}}{c})}$$
\\
De plus la d\'eriv\'ee seconde de la log-vraisemblance sur $\theta$, pour tout $\theta$:
$$\frac{\partial^{2} l_{n}}{\partial^{2} \theta}(x_{1};...;x_{n};\theta)=\frac{-n}{\theta^{2}} <0 $$
Donc le point trouv\'e pr\'ec\'edemment est bien un maximum.\\

Ainsi l'estimateur du maximum de vraisemblance est:
$$\theta^{emv}=\frac{n}{\sum_{i=1}^{n}\ln(\frac{x_{i}}{c})}$$

\subsubsection*{Question 2}
Dans le cas, $\theta < 1$, on cherche un estimateur param\`etrique de l'esp\ 'erance m de la loi de Pareto. On prend $c=1$. On a:
\begin{align*}
\hat{m} &= \frac{\theta ^{emv}}{\theta ^{emv}-1}\times 1\\
		&= \frac{\frac{n}{\sum_{i=1}^{n}\ln(\frac{x_{i}}{c})}}{\frac{n}{\sum_{i=1}^{n}\ln(\frac{x_{i}}{c})} -1}\\
		&= \frac{n}{n-\sum_{i=1}^{n}\ln(\frac{x_{i}}{c})}
\end{align*}
\\
Cherchons maintenant la loi asymptotique de cet estimateur.\\
On se concentre tout d'abord sur l'estimateur $\theta ^{emv}$. On note $f$ la densit\'e de la loir de Pareto. On a pour tout $x>c$,:
$$f(x;\theta)=\theta\frac{c^{\theta}}{x^{\theta+1}}$$
Calculons l'information de Fisher de $\theta$:
\begin{align*}
I_{n}(\theta)&= - \mathbb{E}_{\theta}[\frac{\partial^{2} l_{n}}{\partial^{2} \theta}(x_{1};...;x_{n};\theta)]\\
		&= \mathbb{E}_{\theta}[\frac{n}{\theta^{2}}]\\
		&= \frac{n}{\theta^{2}}
\end{align*}


On remarque donc que:
\begin{enumerate}
	\item Le support de $f(.,\theta)$ est le m\^eme pour tout $\theta$ puisqu'il ne d\' epend que de $c$.
	\item $\theta_{0} (=3)$ est un point int\'erieur de $\Theta$
	\item Pour tout $x$, et \`a proximit\'e de $\theta_{0}$, $f$ est trois fois d\'erivale en $\theta$ et vaut $\frac{2}{\theta^{3}}$ que l'on peut majorer par la fonction constante \' egale \`a 2. De plus si on pose $M:x\longmapsto 2$, on a $\mathbb{E}_{\theta_{0}}(M(X))=2<+\infty$
	\item On a $I_{1}(\theta)= \frac{1}{\theta^{2}}>0$
\end{enumerate}
Ainsi, par th\'eor\^eme, on a que la solution de  l'\'equation de vraisemblance $\theta^{emv}$ v\'erifie:
$$\sqrt{n}(\theta^{emv}-\theta_{0})\stackrel{\mathcal{D}}{\longrightarrow}\mathcal{N}(0,\theta^{2}) \ \  [\mathbb{P}_{\theta_{0}}]$$
\\
On veut ensuite appliquer la Delta-method pour obtenir la loi asymptotique de l'estimateur de $m$.
\\Posons $g:x \longmapsto \frac{x}{1-x}$ d\'efini sur $\mathopen{]}1\,;+\infty\mathclose{]}$.
\\ On a, pour tout $\theta>1$:
$$g'(\theta)=\frac{1}{(\theta-1)^{2}}\neq0$$
Et comme, $\hat{m}=g(\theta^{emv})$, on sait que le $\hat{m}$ ainsi d√©fini est un estimateur du maximum de vraisemblance de $m$, et on obtient:
$$\sqrt{n}(g(\theta^{emv})-g(\theta_{0}))\stackrel{\mathcal{D}}{\longrightarrow}\mathcal{N}(0,g'(\theta)^{2}\theta^{2}) \ \  [\mathbb{P}_{\theta_{0}}]$$
D'o\`u:
$$\sqrt{n}(\hat{m}-m)\stackrel{\mathcal{D}}{\longrightarrow}\mathcal{N}(0,\frac{\theta^{2}}{(\theta-1)^{4}}) \ \  [\mathbb{P}_{\theta_{0}}]$$
\\
\subsubsection{Question 3}
D'apr\`es le r\'esultat de la question pr\'ec\'edente, on a :
$$\sqrt{n}(\hat{m}-m)\stackrel{\mathcal{D}}{\longrightarrow}\mathcal{N}(0,\frac{\theta^{2}}{(\theta-1)^{4}}) \ \  [\mathbb{P}_{\theta_{0}}]$$

		
De plus, on sait que $\theta^{emv}\stackrel{p.s.}{\longrightarrow}\theta\ \  [\mathbb{P}_{\theta_{0}}] $ .\\
Donc, on a: 
$$\sqrt{\frac{(\theta^{emv}-1)^{4}}{(\theta^{emv})^{2}}} \stackrel{p.s.}{\longrightarrow}  \sqrt{\frac{(\theta-1)^{4}}{(\theta)^{2}}}\ \  [\mathbb{P}_{\theta_{0}}] $$

Or, la convergence presque s\^ure entraine la convergence en probabilit\'e.\\

On obtient ainsi:

$$\frac{(\theta^{emv}-1)^{2}}{\theta^{emv}} \stackrel{P}{\longrightarrow}  \frac{(\theta-1)^{2}}{\theta}\ \  [\mathbb{P}_{\theta_{0}}] $$
\\

Enfin, la fonction $h:(x,y)\longmapsto xy$ est une fonction continue sur $\mathbb{R}$.\\


On peut donc appliquer le th\' eor\`eme de Slutsky et obtenir le r\'esultat suivant:

$$\frac{(\theta^{emv}-1)^{2}}{\theta^{emv}}\times s\sqrt{n}(\hat{m}-m)\stackrel{\mathcal{D}}{\longrightarrow}\mathcal{N}(0,1) \ \  [\mathbb{P}_{\theta_{0}}]$$

On peut alors construire un intervalle de confiance asymptotique pour le seuil $\alpha$, en notant $u_{\alpha} $ le quantile $(1-\frac{\alpha}{2})$ d'une loi normale centr\'ee r\'eduite. En effet:
\begin{align*}
1-\alpha &= \lim_{n\rightarrow =\infty}\mathbb{P}(-u_{\alpha}\leq\frac{(\theta^{emv}-1)^{2}}{\theta^{emv}}\times\sqrt{n}(\hat{m}-m)\leq u_{\alpha})\\
		&= \lim_{n\rightarrow =\infty}\mathbb{P}(\hat{m}+\frac{u_{\alpha}\theta^{emv}}{\sqrt{n}(\theta^{emv}-1)^{2}}\geq m \geq \hat{m}-\frac{u_{\alpha}\theta^{emv}}{\sqrt{n}(\theta^{emv}-1)^{2}})
\end{align*}

On obtient ainsi l'intervalle de confiance suivant:
\begin{align*}
&[\hat{m}+\frac{u_{\alpha}\theta^{emv}}{\sqrt{n}(\theta^{emv}-1)^{2}}; \hat{m}-\frac{u_{\alpha}\theta^{emv}}{\sqrt{n}(\theta^{emv}-1)^{2}}]
\\&=[\hat{m}(1+\frac{u_{\alpha}}{\sqrt{n}(\theta^{emv}-1)}); \hat{m}(1-\frac{u_{\alpha}}{\sqrt{n}(\theta^{emv}-1)})]\\
		&=[\frac{1}{1-\frac{\sum_{i=1}^{n}\ln(\frac{x_{i}}{c})}{n}}(1+\frac{u_{\alpha}}{\sqrt{n}(\frac{n}{\sum_{i=1}^{n}\ln(\frac{x_{i}}{c})}-1)}); \frac{1}{1-\frac{\sum_{i=1}^{n}\ln(\frac{x_{i}}{c})}{n}}(1-\frac{u_{\alpha}}{\sqrt{n}(\frac{n}{\sum_{i=1}^{n}\ln(\frac{x_{i}}{c})}-1)})]		
\end{align*}



%Pour $\alpha = 0,05$, on a $u_{\alpha}=1,96$. On obtient ainsi:


\end{document}
